{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17b30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Caricamento e Pulizia del Dataset ---\n",
      "Righe dopo la rimozione degli outlier: 16312 (rimossi 4328)\n",
      "\n",
      "--- 2. Feature Engineering Avanzato ---\n",
      "Nuove Feature Aggiunte: lon_from_coast, bedrooms_per_room, pop_per_house, pop_per_cell, position\n",
      "\n",
      "--- 3. Preprocessing: Standardizzazione e Split ---\n",
      "Train set (Full): 13049 campioni | Test set: 3263 campioni\n",
      "\n",
      "--- 4. Ottimizzazione Iperparametri (20 trial per modello) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-09 16:54:04,992] Trial 19 failed with parameters: {'n_estimators': 1040, 'learning_rate': 0.028759961176147535, 'max_depth': 9, 'subsample': 0.7695337626923384, 'colsample_bytree': 0.7263155741070259} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rosy\\AppData\\Local\\Temp\\ipykernel_10348\\1307820652.py\", line 174, in objective_xgb\n",
      "    model.fit(X_opt_train, y_opt_train, verbose=False)\n",
      "  File \"c:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1368, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 2434, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-09 16:54:05,000] Trial 19 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(mean_squared_error(y_opt_val, preds))\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Esecuzione Ottimizzazione\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m study_xgb = \u001b[43moptuna\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mminimize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m study_cat = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m).optimize(objective_cat, n_trials=\u001b[32m20\u001b[39m)\n\u001b[32m    213\u001b[39m study_lgbm = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m).optimize(objective_lgbm, n_trials=\u001b[32m20\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mobjective_xgb\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    163\u001b[39m params = {\n\u001b[32m    164\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m600\u001b[39m, \u001b[32m1200\u001b[39m),\n\u001b[32m    165\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.01\u001b[39m, \u001b[32m0.08\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    171\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    172\u001b[39m }\n\u001b[32m    173\u001b[39m model = XGBRegressor(**params)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_opt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_opt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m preds = model.predict(X_opt_val)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(mean_squared_error(y_opt_val, preds))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1368\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1366\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rosy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# California Housing Advanced ML Pipeline con Stacking e Optuna (CPU-based)\n",
    "# Obiettivo: Ridurre l'RMSE a <= 0.32 sfruttando l'Ensemble Stacking.\n",
    "\n",
    "## 0. Installazione Librerie Aggiuntive (Necessaria per LightGBM e CatBoost)\n",
    "import sys\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\", \"catboost\", \"shap\", \"--quiet\"])\n",
    "except Exception as e:\n",
    "    print(f\"Attenzione: Impossibile installare LightGBM/CatBoost/SHAP: {e}\")\n",
    "\n",
    "## import delle librerie\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost.callback import EarlyStopping\n",
    "import shap\n",
    "import warnings\n",
    "# Configurazione\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "## 1. Caricamento e Preparazione del Dataset\n",
    "print(\"--- 1. Caricamento e Pulizia del Dataset ---\")\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(housing.data, columns = housing.feature_names)\n",
    "df['med_house_val'] = housing.target\n",
    "\n",
    "# Pulizia e Rinominazione delle colonne\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "df = df.rename(columns={\n",
    "    \"medinc\": \"med_inc\",\n",
    "    \"houseage\": \"house_age\",\n",
    "    \"averooms\": \"ave_rooms\",\n",
    "    \"avebedrms\": \"ave_bedrms\",\n",
    "    \"aveoccup\": \"ave_occup\",\n",
    "})\n",
    "\n",
    "# Outlier Detection (Metodo IQR, k=1.5)\n",
    "num_features = ['med_inc', 'house_age', 'ave_rooms', 'ave_bedrms', 'population', 'ave_occup', 'med_house_val']\n",
    "\n",
    "def get_outlier_mask(dataframe, cols=num_features, k=1.5):\n",
    "    mask = pd.DataFrame(False, index=dataframe.index, columns=cols)\n",
    "    for col in cols:\n",
    "        q1 = dataframe[col].quantile(0.25)\n",
    "        q3 = dataframe[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - k*iqr\n",
    "        upper = q3 + k*iqr\n",
    "        mask[col] = (dataframe[col] < lower) | (dataframe[col] > upper)\n",
    "    return mask.any(axis=1)\n",
    "\n",
    "before_count = df.shape[0]\n",
    "mask = get_outlier_mask(df)\n",
    "df = df[~mask].dropna()\n",
    "after_count = df.shape[0]\n",
    "\n",
    "print(f\"Righe dopo la rimozione degli outlier: {after_count} (rimossi {before_count - after_count})\")\n",
    "\n",
    "\n",
    "## 2. Feature Engineering\n",
    "print(\"\\n--- 2. Feature Engineering Avanzato ---\")\n",
    "# Punti di ancoraggio per il fit quadratico (Curva della Costa)\n",
    "_LAT = np.array([41.8, 39.0, 37.7, 36.6, 34.4, 34.0, 32.7])\n",
    "_LON = np.array([-124.2, -123.7, -122.5, -121.9, -120.3, -118.5, -117.2])\n",
    "_COAST_COEFFS = np.polyfit(_LAT, _LON, deg=2)\n",
    "\n",
    "def _coast_lon(lat):\n",
    "    a, b, c = _COAST_COEFFS\n",
    "    return a*lat*lat + b*lat + c\n",
    "\n",
    "def classify_location(lat, lon):\n",
    "    coast_at_lat = _coast_lon(lat)\n",
    "    coast_buffer = 1\n",
    "    inland_shift = 2.3\n",
    "    inland_at_lat = coast_at_lat + inland_shift\n",
    "\n",
    "    if lon <= coast_at_lat + coast_buffer:\n",
    "        return \"coast\"\n",
    "    elif lon >= inland_at_lat:\n",
    "        return \"inland\"\n",
    "    else:\n",
    "        return \"middle\"\n",
    "\n",
    "df[\"position\"] = df.apply(lambda row: classify_location(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "# Distanza Longitudinale dalla Costa\n",
    "df[\"lon_from_coast\"] = df[\"longitude\"] - df.apply(lambda row: _coast_lon(row['latitude']), axis=1)\n",
    "\n",
    "# Rapporti e Densità\n",
    "df[\"bedrooms_per_room\"] = df[\"ave_bedrms\"] / df[\"ave_rooms\"]\n",
    "df[\"rooms_per_house\"] = df[\"ave_rooms\"]\n",
    "df[\"pop_per_house\"] = df[\"ave_occup\"]\n",
    "\n",
    "# Popolazione per Cella (Densità Locale)\n",
    "def add_population_per_cell(df, lat_col=\"latitude\", lon_col=\"longitude\", pop_col=\"population\",\n",
    "                            lat_bin=0.1, lon_bin=0.1):\n",
    "    df = df.copy()\n",
    "    df[\"_lat_bin\"] = (df[lat_col] // lat_bin) * lat_bin\n",
    "    df[\"_lon_bin\"] = (df[lon_col] // lon_bin) * lon_bin\n",
    "    \n",
    "    pop_map = (\n",
    "        df.groupby([\"_lat_bin\", \"_lon_bin\"])[pop_col]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={pop_col: \"pop_per_cell\"})\n",
    "    )\n",
    "    \n",
    "    df = df.merge(pop_map, on=[\"_lat_bin\", \"_lon_bin\"], how=\"left\")\n",
    "    return df.drop(columns=[\"_lat_bin\", \"_lon_bin\"])\n",
    "\n",
    "df = add_population_per_cell(df)\n",
    "print(f\"Nuove Feature Aggiunte: lon_from_coast, bedrooms_per_room, pop_per_house, pop_per_cell, position\")\n",
    "\n",
    "\n",
    "## 3. Preprocessing: Standardizzazione e Split\n",
    "print(\"\\n--- 3. Preprocessing: Standardizzazione e Split ---\")\n",
    "X = df.drop(columns = ['med_house_val'])\n",
    "y = df[\"med_house_val\"]\n",
    "\n",
    "# Prepariamo i dati per lo Stacking (non è richiesta la standardizzazione per i Tree Models,\n",
    "# ma è meglio farla per il Meta-Learner se fosse una Regressione Lineare e per coerenza con il tuo codice precedente)\n",
    "X_numeric = X.drop(columns=['position'], errors='ignore')\n",
    "\n",
    "# Standardizzazione\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_numeric.columns)\n",
    "y = df[\"med_house_val\"].reset_index(drop=True) \n",
    "\n",
    "# Divisione train/test (80-20)\n",
    "# Usiamo i dati non standardizzati per lo Stacking (i Tree Models funzionano meglio sulle feature native)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_numeric, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train set (Full): {len(X_train_full)} campioni | Test set: {len(X_test)} campioni\")\n",
    "\n",
    "# Split Secondario per Optuna\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42 # 25% del 80% = 20% totale\n",
    ")\n",
    "\n",
    "\n",
    "## 4. Ottimizzazione Iperparametri con Optuna (Light - 20 prove per modello)\n",
    "print(\"\\n--- 4. Ottimizzazione Iperparametri (20 trial per modello) ---\")\n",
    "\n",
    "# --- XGBoost Objective ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'objective': 'reg:squarederror'\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train, verbose=False)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "# --- CatBoost Objective ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "# --- LightGBM Objective ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "# Esecuzione Ottimizzazione\n",
    "study_xgb = optuna.create_study(direction='minimize').optimize(objective_xgb, n_trials=20)\n",
    "study_cat = optuna.create_study(direction='minimize').optimize(objective_cat, n_trials=20)\n",
    "study_lgbm = optuna.create_study(direction='minimize').optimize(objective_lgbm, n_trials=20)\n",
    "\n",
    "# Raccolta dei migliori parametri\n",
    "best_xgb_params = study_xgb.best_params.copy()\n",
    "best_xgb_params.update({'n_jobs': -1, 'random_state': 42, 'objective': 'reg:squarederror'})\n",
    "\n",
    "best_cat_params = study_cat.best_params.copy()\n",
    "best_cat_params.update({'verbose': 0, 'allow_writing_files': False, 'random_state': 42})\n",
    "\n",
    "best_lgbm_params = study_lgbm.best_params.copy()\n",
    "best_lgbm_params.update({'n_jobs': -1, 'verbose': -1, 'random_state': 42})\n",
    "\n",
    "print(f\"\\nBest RMSE Val -> XGB: {study_xgb.best_value:.4f} | CAT: {study_cat.best_value:.4f} | LGBM: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "## 5. Stacking Regressor: Level 0 e Level 1\n",
    "print(\"\\n--- 5. Costruzione Stacking Regressor ---\")\n",
    "\n",
    "# Definizione Level 0 (Base Models)\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params))]\n",
    "\n",
    "# Definizione Level 1 (Meta Model)\n",
    "meta_learner = RidgeCV(alphas=[0.1, 1.0, 10.0]) # Ridge con Cross-Validation per trovare il miglior alpha\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,            # 5-Fold per generare previsioni out-of-fold robuste\n",
    "    n_jobs=-1,\n",
    "    passthrough=False # Il Meta-Learner prende in input SOLO le predizioni Level 0\n",
    ")\n",
    "\n",
    "print(\"Addestramento Stacking in corso...\")\n",
    "stacking_regressor.fit(X_train_full, y_train_full) # Fit sul training set completo\n",
    "\n",
    "\n",
    "## 6. Valutazione Finale Stacking Ensemble\n",
    "print(\"\\n--- 6. Valutazione Finale Ensemble ---\")\n",
    "\n",
    "preds_stacking = stacking_regressor.predict(X_test)\n",
    "rmse_stacking = np.sqrt(mean_squared_error(y_test, preds_stacking))\n",
    "r2_stacking = r2_score(y_test, preds_stacking)\n",
    "\n",
    "# Aggiungi a results (per coerenza con il codice precedente)\n",
    "results.append({'Modello': 'Ensemble (Stacking)', 'R²': r2_stacking, 'RMSE': rmse_stacking, 'MAE': mean_absolute_error(y_test, preds_stacking), 'MSE': mean_squared_error(y_test, preds_stacking)})\n",
    "\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"STACKING FINAL RMSE: {rmse_stacking:.4f}\")\n",
    "print(f\"STACKING FINAL R2:   {r2_stacking:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Obiettivo RMSE <= 0.32: {'Sì' if rmse_stacking <= 0.32 else 'No'}\")\n",
    "\n",
    "\n",
    "# Ispezione Pesi Meta-Learner (Importanza)\n",
    "print(\"\\nPesi Assegnati dal Meta-Modello (Level 1):\")\n",
    "model_names = [name.upper() for name, _ in estimators]\n",
    "coeffs = stacking_regressor.final_estimator_.coef_\n",
    "for name, coef in zip(model_names, coeffs):\n",
    "    print(f\"  {name:<10}: {coef:.4f}\")\n",
    "\n",
    "\n",
    "## 7. Visualizzazione Finale e Confronto (Aggiornato)\n",
    "print('\\n--- 7. CONFRONTO FINALE DEI MODELLI ---')\n",
    "results_df = pd.DataFrame(results).drop_duplicates(subset=['Modello'], keep='last').sort_values(by='R²', ascending=False)\n",
    "print(results_df)\n",
    "\n",
    "# Hexbin visualization of STACKING predictions\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "hb = ax.hexbin(y_test, preds_stacking, gridsize=30, cmap='viridis', mincnt=1)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "cb = plt.colorbar(hb, ax=ax)\n",
    "cb.set_label('Densità Campioni')\n",
    "ax.set_xlabel('Prezzo Reale')\n",
    "ax.set_ylabel('Prezzo Predetto')\n",
    "ax.set_title(f'Predizioni Stacking Ensemble\\nR²={r2_stacking:.4f}, RMSE={rmse_stacking:.4f}')\n",
    "plt.show()\n",
    "\n",
    "print('\\nPipeline completata con Feature Engineering avanzata e Stacking Ensemble ottimizzato.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
